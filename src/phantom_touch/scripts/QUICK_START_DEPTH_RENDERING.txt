================================================================================
PHANTOM-TOUCH TO ONTOUCH DEPTH RENDERING - QUICK START GUIDE
================================================================================

OVERVIEW:
This integration renders depth patches using MuJoCo for your Phantom-Touch
dataset, combining object trajectories, robot motion, and camera extrinsics.

================================================================================
STEP 1: ENSURE 3D TRACKING IS COMPLETE
================================================================================

Check if tracking results exist:
  ls /mnt/dataset_drive/ayad/data/phantom-touch/data/output/handover_collection_0/sam2-vid_output/absolute_positions.npy

If not found, run tracking first:
  cd /home/epon04yc/phantom-touch/src/phantom-touch/scripts
  python threeD_tracking_offline.py

================================================================================
STEP 2: RUN THE COMPLETE PIPELINE
================================================================================

  cd /home/epon04yc/phantom-touch/src/phantom-touch/scripts
  ./phantom_to_ontouch_pipeline.sh

This will:
  ✓ Create banana CAD model (if needed)
  ✓ Setup MuJoCo scene with banana
  ✓ Render RGB and depth from 3 cameras
  ✓ Generate visualizations

================================================================================
STEP 3: VIEW RESULTS
================================================================================

Output location:
  /mnt/dataset_drive/ayad/data/phantom-touch/data/output/handover_collection_0/mujoco_depth_renders/

Files created:
  - frame_XXXX_[camera]_rgb.png (RGB images)
  - frame_XXXX_[camera]_depth.png (depth in mm as uint16)
  - render_manifest.npz (metadata)
  - visualizations/ (sample images and videos)

Download visualizations to local machine:
  scp -r user@server:/path/to/mujoco_depth_renders/visualizations ./

================================================================================
CAMERAS
================================================================================

Three cameras are rendered per frame:

1. orbbec - Main RGB-D camera
   - Position: (0.600, -0.194, 1.201) m
   - FOV: 51°
   - Calibrated extrinsics

2. cam_left_digit - Left tactile finger
   - FOV: 60°
   - For contact visualization

3. cam_right_digit - Right tactile finger  
   - FOV: 60°
   - For contact visualization

================================================================================
CONFIGURATION
================================================================================

Edit settings (if needed):
  nano config/depth_patch_renderer_phantom.yaml

Key settings:
  - phantom_dataset_root: Dataset path
  - render_width/height: Output resolution (240x320)
  - cameras: List of cameras to render

================================================================================
TROUBLESHOOTING
================================================================================

Issue: "Object tracking not found"
Fix: Run threeD_tracking_offline.py first

Issue: "Banana CAD model not found"  
Fix: Run python download_banana_model.py

Issue: "MuJoCo error"
Fix: Check that OnTouch repo is at /home/epon04yc/ontouch

Issue: "No robot trajectory"
Fix: Optional - system uses static pose if not available

================================================================================
WHAT'S HAPPENING UNDER THE HOOD
================================================================================

The pipeline:

1. Loads your tracked object poses (position + orientation)
2. Loads robot joint trajectories (if available)
3. For each frame:
   - Sets robot configuration
   - Sets banana pose from tracking
   - Renders from each camera
   - Saves RGB and depth (mm as uint16)
4. Creates manifest with all metadata

The depth rendering uses MuJoCo's physics simulation with your banana CAD
model, providing realistic depth patches that match the object trajectory
you tracked with SAM2 + ICP.

================================================================================
NEXT STEPS
================================================================================

After rendering:

1. Visualize results:
   python visualize_depth_renders.py

2. Integrate with OnTouch dataset:
   from datasets.ontouch_dataset import DepthToTouchDataset
   dataset = DepthToTouchDataset(root_dir="path/to/renders")

3. Train depth-to-touch models using OnTouch pipeline

================================================================================
NEED HELP?
================================================================================

See detailed documentation:
  cat README_DEPTH_RENDERING.md

Check OnTouch original implementation:
  /home/epon04yc/ontouch/depth_patch_renderer_session/

================================================================================
