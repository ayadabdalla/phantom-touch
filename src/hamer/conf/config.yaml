checkpoint: ~/phantom-touch/src/hamer/_DATA/hamer_ckpts/checkpoints/hamer.ckpt # Path to pretrained model checkpoint
img_folder: /mnt/dataset_drive/ayad/phantom-touch/data/recordings/white_cloth_exp/white_nonreflective_cloth_light_on_ambient_light/png_output/color_sample/ # Folder with input images
out_folder: /mnt/dataset_drive/ayad/phantom-touch/data/output/white_cloth_exp/white_nonreflective_cloth_light_on_ambient_light/hamer_output/ # Output folder to save rendered results
side_view: true # If set to true, render side view also
full_frame: true # If set to true, render all people together also
save_mesh: true # If set to true, save meshes to disk also
batch_size: 48 # Batch size for inference/fitting
rescale_factor: 2.0 # Factor for padding the bbox
body_detector: vitdet # Using regnety improves runtime and reduces memory; choices: ['vitdet', 'regnety']
file_type: ['*.jpg', '*.png']